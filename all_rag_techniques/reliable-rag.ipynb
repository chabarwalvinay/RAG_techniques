{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-20T14:25:04.186903Z",
     "iopub.status.busy": "2025-01-20T14:25:04.186455Z",
     "iopub.status.idle": "2025-01-20T14:25:04.567449Z",
     "shell.execute_reply": "2025-01-20T14:25:04.566641Z",
     "shell.execute_reply.started": "2025-01-20T14:25:04.186871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-20T14:25:04.569239Z",
     "iopub.status.busy": "2025-01-20T14:25:04.568727Z",
     "iopub.status.idle": "2025-01-20T14:25:52.715308Z",
     "shell.execute_reply": "2025-01-20T14:25:52.714070Z",
     "shell.execute_reply.started": "2025-01-20T14:25:04.569203Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain chromadb cohere\n",
    "!pip install -U langchain-cohere\n",
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:26:29.211071Z",
     "iopub.status.busy": "2025-01-20T14:26:29.210708Z",
     "iopub.status.idle": "2025-01-20T14:26:29.215723Z",
     "shell.execute_reply": "2025-01-20T14:26:29.214667Z",
     "shell.execute_reply.started": "2025-01-20T14:26:29.211043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:26:29.418005Z",
     "iopub.status.busy": "2025-01-20T14:26:29.417598Z",
     "iopub.status.idle": "2025-01-20T14:26:30.233644Z",
     "shell.execute_reply": "2025-01-20T14:26:30.232556Z",
     "shell.execute_reply.started": "2025-01-20T14:26:29.417972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "COHERE_API_KEY = user_secrets.get_secret(\"COHERE_API_KEY\")\n",
    "GEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:26:30.235388Z",
     "iopub.status.busy": "2025-01-20T14:26:30.235063Z",
     "iopub.status.idle": "2025-01-20T14:26:32.178556Z",
     "shell.execute_reply": "2025-01-20T14:26:32.177396Z",
     "shell.execute_reply.started": "2025-01-20T14:26:30.235346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set embeddings\n",
    "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\", cohere_api_key=COHERE_API_KEY, user_agent=\"my_project\")\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:26:32.598760Z",
     "iopub.status.busy": "2025-01-20T14:26:32.598364Z",
     "iopub.status.idle": "2025-01-20T14:26:32.602729Z",
     "shell.execute_reply": "2025-01-20T14:26:32.601704Z",
     "shell.execute_reply.started": "2025-01-20T14:26:32.598731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"what are the different kind of agentic design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:26:41.494964Z",
     "iopub.status.busy": "2025-01-20T14:26:41.494615Z",
     "iopub.status.idle": "2025-01-20T14:26:41.748777Z",
     "shell.execute_reply": "2025-01-20T14:26:41.747652Z",
     "shell.execute_reply.started": "2025-01-20T14:26:41.494937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:27:06.497158Z",
     "iopub.status.busy": "2025-01-20T14:27:06.496708Z",
     "iopub.status.idle": "2025-01-20T14:27:06.502292Z",
     "shell.execute_reply": "2025-01-20T14:27:06.500932Z",
     "shell.execute_reply.started": "2025-01-20T14:27:06.497124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
      "\n",
      "Source: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
      "\n",
      "Content: GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Title: {docs[0].metadata['title']}\\n\\nSource: {docs[0].metadata['source']}\\n\\nContent: {docs[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check document relevancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:35:15.174789Z",
     "iopub.status.busy": "2025-01-20T14:35:15.174356Z",
     "iopub.status.idle": "2025-01-20T14:35:15.304984Z",
     "shell.execute_reply": "2025-01-20T14:35:15.304020Z",
     "shell.execute_reply.started": "2025-01-20T14:35:15.174757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0, api_key=GROQ_API_KEY)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out the non-relevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:40:41.545120Z",
     "iopub.status.busy": "2025-01-20T14:40:41.544656Z",
     "iopub.status.idle": "2025-01-20T14:40:42.509683Z",
     "shell.execute_reply": "2025-01-20T14:40:42.508630Z",
     "shell.execute_reply.started": "2025-01-20T14:40:41.545090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"ShareSubscribe to The BatchStay updated with weekly AI News and Insights delivered to your inboxCoursesThe BatchCommunityCareersAbout \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Retrieved 3 documents.\n"
     ]
    }
   ],
   "source": [
    "docs_to_use = []\n",
    "max_docs = 3  # Limit to 3 documents\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    if idx >= max_docs:  # Break the loop if we've already processed 3 documents\n",
    "        break\n",
    "    \n",
    "    print(doc.page_content, '\\n', '-'*50)\n",
    "    res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    print(res, '\\n')\n",
    "    \n",
    "    if res.binary_score == 'yes':\n",
    "        docs_to_use.append(doc)\n",
    "\n",
    "# Print the number of docs retrieved\n",
    "print(f\"Retrieved {len(docs_to_use)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:41:12.246790Z",
     "iopub.status.busy": "2025-01-20T14:41:12.246335Z",
     "iopub.status.idle": "2025-01-20T14:41:12.252170Z",
     "shell.execute_reply": "2025-01-20T14:41:12.250871Z",
     "shell.execute_reply.started": "2025-01-20T14:41:12.246750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# docs_to_use = []\n",
    "# max_docs = 3  # Set the maximum number of documents to retrieve\n",
    "# retries = 0\n",
    "# max_retries = 5  # Set the max retries for error handling\n",
    "\n",
    "# for doc in docs:\n",
    "#     if len(docs_to_use) >= max_docs:\n",
    "#         break  # Stop if we've already retrieved 3 documents\n",
    "    \n",
    "#     print(doc.page_content, '\\n', '-'*50)\n",
    "    \n",
    "#     success = False\n",
    "#     while retries < max_retries and not success:\n",
    "#         try:\n",
    "#             # Call the API to get the result for the document\n",
    "#             res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#             print(res, '\\n')\n",
    "            \n",
    "#             # If the binary_score is 'yes', append the document to docs_to_use\n",
    "#             if res.get('binary_score') == 'yes':  # Adjust based on response format\n",
    "#                 docs_to_use.append(doc)\n",
    "            \n",
    "#             success = True  # Mark as successful if no error occurs\n",
    "#         except Exception as e:\n",
    "#             # If an error occurs (e.g., rate limit), retry with delay\n",
    "#             print(f\"Error encountered: {e}. Retrying...\")\n",
    "#             retries += 1\n",
    "#             delay = min(2 ** retries, 60)  # Exponential backoff with a max delay of 60 seconds\n",
    "#             print(f\"Retrying after {delay} seconds...\")\n",
    "#             time.sleep(delay)  # Wait before retrying\n",
    "\n",
    "#     # If we've successfully retrieved 3 documents, stop further processing\n",
    "#     if len(docs_to_use) >= max_docs:\n",
    "#         break\n",
    "\n",
    "# if len(docs_to_use) < max_docs:\n",
    "#     print(f\"Retrieved {len(docs_to_use)} documents, fewer than {max_docs} due to errors or retries.\")\n",
    "# else:\n",
    "#     print(f\"Successfully retrieved {max_docs} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:42:01.511985Z",
     "iopub.status.busy": "2025-01-20T14:42:01.511625Z",
     "iopub.status.idle": "2025-01-20T14:42:02.196956Z",
     "shell.execute_reply": "2025-01-20T14:42:02.195893Z",
     "shell.execute_reply.started": "2025-01-20T14:42:01.511959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the retrieved documents, there are four main agentic design patterns mentioned:\n",
      "\n",
      "1. Reflection: The LLM examines its own work to come up with ways to improve it.\n",
      "2. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.\n",
      "3. Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal.\n",
      "4. Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0, api_key = GROQ_API_KEY)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:43:01.446822Z",
     "iopub.status.busy": "2025-01-20T14:43:01.446419Z",
     "iopub.status.idle": "2025-01-20T14:43:01.990213Z",
     "shell.execute_reply": "2025-01-20T14:43:01.989361Z",
     "shell.execute_reply.started": "2025-01-20T14:43:01.446792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        ...,\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0, api_key = GROQ_API_KEY)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "response = hallucination_grader.invoke({\"documents\": format_docs(docs_to_use), \"generation\": generation})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlight the used docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:44:05.906266Z",
     "iopub.status.busy": "2025-01-20T14:44:05.905927Z",
     "iopub.status.idle": "2025-01-20T14:44:07.183031Z",
     "shell.execute_reply": "2025-01-20T14:44:07.182007Z",
     "shell.execute_reply.started": "2025-01-20T14:44:05.906242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Data model\n",
    "class HighlightDocuments(BaseModel):\n",
    "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
    "\n",
    "    id: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of id of docs used to answers the question\"\n",
    "    )\n",
    "\n",
    "    title: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of titles used to answers the question\"\n",
    "    )\n",
    "\n",
    "    source: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of sources used to answers the question\"\n",
    "    )\n",
    "\n",
    "    segment: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of direct segements from used documents that answers the question\"\n",
    "    )\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0, api_key = GROQ_API_KEY)\n",
    "\n",
    "# parser\n",
    "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
    "1. A question.\n",
    "2. A generated answer based on the question.\n",
    "3. A set of documents that were referenced in generating the answer.\n",
    "\n",
    "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to \n",
    "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text \n",
    "in the provided documents.\n",
    "\n",
    "Ensure that:\n",
    "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
    "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
    "- (Important) If you didn't used the specific document don't mention it.\n",
    "\n",
    "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
    "\n",
    "<format_instruction>\n",
    "{format_instructions}\n",
    "</format_instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= system,\n",
    "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain\n",
    "doc_lookup = prompt | llm | parser\n",
    "\n",
    "# Run\n",
    "lookup_response = doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T14:44:19.285680Z",
     "iopub.status.busy": "2025-01-20T14:44:19.285273Z",
     "iopub.status.idle": "2025-01-20T14:44:19.290730Z",
     "shell.execute_reply": "2025-01-20T14:44:19.289569Z",
     "shell.execute_reply.started": "2025-01-20T14:44:19.285650Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc1\n",
      "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
      "Source: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.\n",
      "\n",
      "ID: doc2\n",
      "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
      "Source: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.\n",
      "\n",
      "ID: doc3\n",
      "Title: Four AI Agent Strategies That Improve GPT-4 and GPT-3.5 Performance\n",
      "Source: https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id, title, source, segment in zip(lookup_response.id, lookup_response.title, lookup_response.source, lookup_response.segment):\n",
    "    print(f\"ID: {id}\\nTitle: {title}\\nSource: {source}\\nText Segment: {segment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
